{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import ssl\n",
    "from tensorflow import keras\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "print('Tensorflow Version: ', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: SK-Learn (Fill In)\n",
    "\n",
    "SK-Learn Documentation: https://scikit-learn.org/stable/ <br> \n",
    "Geeks4Geeks (More specific to this example): https://www.geeksforgeeks.org/python-linear-regression-using-sklearn/#\n",
    "\n",
    "Please don't use chatgpt :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CRIM     per capita crime rate by town\n",
    "#  ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "#  INDUS    proportion of non-retail business acres per town\n",
    "#  CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "#  NOX      nitric oxides concentration (parts per 10 million)\n",
    "#  RM       average number of rooms per dwelling\n",
    "#  AGE      proportion of owner-occupied units built prior to 1940\n",
    "#  DIS      weighted distances to five Boston employment centres\n",
    "#  RAD      index of accessibility to radial highways\n",
    "#  TAX      full-value property-tax rate per $10,000\n",
    "#  PTRATIO  pupil-teacher ratio by town\n",
    "#  B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "#  LSTAT    % lower status of the population\n",
    "#  MEDV     Median value of owner-occupied homes in $1000's\n",
    "\n",
    "boston_df = pd.DataFrame(data, columns=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'])\n",
    "boston_df['PRICE'] = target\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation matrix\n",
    "corr_matrix = boston_df.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the blank here :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = ... #(Features, target, optimal test size)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(y_train, bins=30, color='blue', label='Training Set', alpha=0.7)\n",
    "sns.histplot(y_test, bins=30, color='red', label='Testing Set', alpha=0.7)\n",
    "plt.title('Distribution of Prices in Training and Testing Sets')\n",
    "plt.xlabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the blank also here :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "...                         #How do we train a model in SK-learn?\n",
    "predictions = ...           #Get our predictions from the model\n",
    "mse = ...                   #How would we calculate mse?\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Visualize predicted vs actual prices\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(y_test, predictions, alpha=0.7)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2, label='Perfect Prediction')\n",
    "plt.title('Actual Prices vs Predicted Prices')\n",
    "plt.xlabel('Actual Prices')\n",
    "plt.ylabel('Predicted Prices')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Keras (Just for Show on MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist # keras provides several datasets available within the library itself\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data() # assigning the data\n",
    "\n",
    "plt.imshow(x_train[0], cmap = plt.cm.binary) # graphical representation of one of the values of the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.utils.normalize(x_train, axis=1) # keras allows you to much more easily normalize you data\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
    "\n",
    "plt.imshow(x_train[0], cmap = plt.cm.binary) # graphical representation of one of the values of the normalized set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(28,28,1))) # converts your data to a one-dimensional array, in this case the images in our set were 28x28\n",
    "model.add(tf.keras.layers.Dense(128,activation='relu')) # adding layers to our model\n",
    "model.add(tf.keras.layers.Dense(128,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(10,activation='softmax')) # final layer should have units equal to the amount of your classification labels\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy']) \n",
    "# optimizers are the functions that tweak the attributes of your neural network,\n",
    "# loss is the function that calculates the difference between predicted and actual values\n",
    "# metrics are the things that you want to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train,epochs=3) # actual training function for our neural network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
